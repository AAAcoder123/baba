{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd044e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install gymnasium[box2d] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7215346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Monte Carlo Prediction on initial policy...\n",
      "Prediction Episode 0/100\n",
      "Prediction Episode 10/100\n",
      "Prediction Episode 20/100\n",
      "Prediction Episode 30/100\n",
      "Prediction Episode 40/100\n",
      "Prediction Episode 50/100\n",
      "Prediction Episode 60/100\n",
      "Prediction Episode 70/100\n",
      "Prediction Episode 80/100\n",
      "Prediction Episode 90/100\n",
      "\n",
      "Training with Monte Carlo Control...\n",
      "Episode 0/2000, Avg Reward: -433.03, Avg Length: 500.0\n",
      "Episode 20/2000, Avg Reward: -438.45, Avg Length: 483.2\n",
      "Episode 40/2000, Avg Reward: -454.51, Avg Length: 497.4\n",
      "Episode 60/2000, Avg Reward: -453.32, Avg Length: 500.0\n",
      "Episode 80/2000, Avg Reward: -434.74, Avg Length: 485.8\n",
      "Episode 100/2000, Avg Reward: -445.31, Avg Length: 496.3\n",
      "Episode 120/2000, Avg Reward: -441.33, Avg Length: 490.2\n",
      "Episode 140/2000, Avg Reward: -453.17, Avg Length: 500.0\n",
      "Episode 160/2000, Avg Reward: -448.17, Avg Length: 497.6\n",
      "Episode 180/2000, Avg Reward: -449.05, Avg Length: 500.0\n",
      "Episode 200/2000, Avg Reward: -444.61, Avg Length: 490.5\n",
      "Episode 220/2000, Avg Reward: -443.42, Avg Length: 493.4\n",
      "Episode 240/2000, Avg Reward: -449.62, Avg Length: 493.0\n",
      "Episode 260/2000, Avg Reward: -444.35, Avg Length: 490.6\n",
      "Episode 280/2000, Avg Reward: -453.80, Avg Length: 500.0\n",
      "Episode 300/2000, Avg Reward: -445.28, Avg Length: 500.0\n",
      "Episode 320/2000, Avg Reward: -433.94, Avg Length: 478.1\n",
      "Episode 340/2000, Avg Reward: -445.79, Avg Length: 493.1\n",
      "Episode 360/2000, Avg Reward: -428.67, Avg Length: 481.8\n",
      "Episode 380/2000, Avg Reward: -448.41, Avg Length: 496.8\n",
      "Episode 400/2000, Avg Reward: -448.38, Avg Length: 498.2\n",
      "Episode 420/2000, Avg Reward: -439.25, Avg Length: 489.8\n",
      "Episode 440/2000, Avg Reward: -439.46, Avg Length: 494.6\n",
      "Episode 460/2000, Avg Reward: -441.99, Avg Length: 494.9\n",
      "Episode 480/2000, Avg Reward: -434.52, Avg Length: 488.3\n",
      "Episode 500/2000, Avg Reward: -445.97, Avg Length: 496.1\n",
      "Episode 520/2000, Avg Reward: -445.15, Avg Length: 495.9\n",
      "Episode 540/2000, Avg Reward: -429.25, Avg Length: 485.1\n",
      "Episode 560/2000, Avg Reward: -438.05, Avg Length: 491.9\n",
      "Episode 580/2000, Avg Reward: -429.50, Avg Length: 483.1\n",
      "Episode 600/2000, Avg Reward: -443.82, Avg Length: 493.4\n",
      "Episode 620/2000, Avg Reward: -435.45, Avg Length: 483.2\n",
      "Episode 640/2000, Avg Reward: -408.81, Avg Length: 460.6\n",
      "Episode 660/2000, Avg Reward: -436.27, Avg Length: 485.1\n",
      "Episode 680/2000, Avg Reward: -393.02, Avg Length: 448.6\n",
      "Episode 700/2000, Avg Reward: -421.69, Avg Length: 476.9\n",
      "Episode 720/2000, Avg Reward: -410.01, Avg Length: 461.4\n",
      "Episode 740/2000, Avg Reward: -431.41, Avg Length: 481.9\n",
      "Episode 760/2000, Avg Reward: -427.20, Avg Length: 479.9\n",
      "Episode 780/2000, Avg Reward: -416.33, Avg Length: 461.6\n",
      "Episode 800/2000, Avg Reward: -426.75, Avg Length: 478.1\n",
      "Episode 820/2000, Avg Reward: -428.52, Avg Length: 479.4\n",
      "Episode 840/2000, Avg Reward: -414.54, Avg Length: 463.1\n",
      "Episode 860/2000, Avg Reward: -423.20, Avg Length: 468.2\n",
      "Episode 880/2000, Avg Reward: -416.59, Avg Length: 469.8\n",
      "Episode 900/2000, Avg Reward: -430.66, Avg Length: 486.8\n",
      "Episode 920/2000, Avg Reward: -419.07, Avg Length: 479.9\n",
      "Episode 940/2000, Avg Reward: -409.40, Avg Length: 460.9\n",
      "Episode 960/2000, Avg Reward: -394.05, Avg Length: 445.4\n",
      "Episode 980/2000, Avg Reward: -423.51, Avg Length: 472.2\n",
      "Episode 1000/2000, Avg Reward: -368.90, Avg Length: 418.4\n",
      "Episode 1020/2000, Avg Reward: -408.98, Avg Length: 463.0\n",
      "Episode 1040/2000, Avg Reward: -423.46, Avg Length: 473.9\n",
      "Episode 1060/2000, Avg Reward: -415.04, Avg Length: 465.0\n",
      "Episode 1080/2000, Avg Reward: -428.01, Avg Length: 479.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "class DiscretizedAcrobot:\n",
    "    def __init__(self, bins=10, render_mode=None):\n",
    "        self.env = gym.make(\"Acrobot-v1\", render_mode=render_mode)\n",
    "        self.bins = bins\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        \n",
    "        self.observation_space_high = np.array([\n",
    "            1.0, 1.0, 1.0, 1.0, 12.0, 28.0\n",
    "        ])\n",
    "        self.observation_space_low = np.array([\n",
    "            -1.0, -1.0, -1.0, -1.0, -12.0, -28.0\n",
    "        ])\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        obs, info = self.env.reset(seed=seed)\n",
    "        return self._discretize_observation(obs), info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        end_height = -np.cos(np.arccos(obs[0])) - np.cos(np.arccos(obs[0]) + np.arccos(obs[2])) + 2.0\n",
    "        enhanced_reward = reward + 0.1 * end_height\n",
    "        \n",
    "        return self._discretize_observation(obs), enhanced_reward, terminated, truncated, info\n",
    "    \n",
    "    def _discretize_observation(self, observation):\n",
    "        scaled = ((observation - self.observation_space_low) / \n",
    "                  (self.observation_space_high - self.observation_space_low) * self.bins)\n",
    "        \n",
    "        discretized = np.clip(scaled.astype(np.int32), 0, self.bins - 1)\n",
    "        \n",
    "        return tuple(discretized)\n",
    "    \n",
    "    def render(self):\n",
    "        return self.env.render()\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "class MonteCarloAgent:\n",
    "    def __init__(self, env, epsilon=0.1, gamma=0.99):\n",
    "        self.env = env\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.num_actions = env.num_actions\n",
    "        \n",
    "        self.Q = defaultdict(lambda: np.zeros(self.num_actions))\n",
    "        self.returns_sum = defaultdict(float)\n",
    "        self.returns_count = defaultdict(int)\n",
    "        self.policy = defaultdict(lambda: np.ones(self.num_actions) / self.num_actions)\n",
    "    \n",
    "    def select_action(self, state, exploring=True):\n",
    "        if exploring and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            return np.argmax(self.Q[state])\n",
    "    \n",
    "    def update_policy(self, state):\n",
    "        best_action = np.argmax(self.Q[state])\n",
    "        for a in range(self.num_actions):\n",
    "            self.policy[state][a] = 1.0 if a == best_action else 0.0\n",
    "    \n",
    "    def generate_episode(self, max_steps=500, exploring=True):\n",
    "        episode = []\n",
    "        state, _ = self.env.reset()\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            action = self.select_action(state, exploring)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "            \n",
    "        return episode\n",
    "    \n",
    "    def monte_carlo_prediction(self, num_episodes=100):\n",
    "        for episode in range(num_episodes):\n",
    "            if episode % 10 == 0:\n",
    "                print(f\"Prediction Episode {episode}/{num_episodes}\")\n",
    "                \n",
    "            episode_data = self.generate_episode()\n",
    "            \n",
    "            G = 0\n",
    "            visited_state_actions = set()\n",
    "            \n",
    "            for t in range(len(episode_data)-1, -1, -1):\n",
    "                state, action, reward = episode_data[t]\n",
    "                G = self.gamma * G + reward\n",
    "                \n",
    "                state_action = (state, action)\n",
    "                if state_action not in visited_state_actions:\n",
    "                    visited_state_actions.add(state_action)\n",
    "                    \n",
    "                    self.returns_sum[state_action] += G\n",
    "                    self.returns_count[state_action] += 1\n",
    "                    self.Q[state][action] = self.returns_sum[state_action] / self.returns_count[state_action]\n",
    "    \n",
    "    def monte_carlo_control(self, num_episodes=1000):\n",
    "        episode_rewards = []\n",
    "        episode_lengths = []\n",
    "        epsilon_schedule = np.linspace(self.epsilon, 0.01, num_episodes)\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            self.epsilon = epsilon_schedule[episode]\n",
    "            \n",
    "            episode_data = self.generate_episode(exploring=True)\n",
    "            episode_reward = sum(r for _, _, r in episode_data)\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_lengths.append(len(episode_data))\n",
    "            \n",
    "            if episode % 20 == 0:\n",
    "                avg_reward = np.mean(episode_rewards[-20:] if episode >= 20 else episode_rewards)\n",
    "                avg_length = np.mean(episode_lengths[-20:] if episode >= 20 else episode_lengths)\n",
    "                print(f\"Episode {episode}/{num_episodes}, Avg Reward: {avg_reward:.2f}, Avg Length: {avg_length:.1f}\")\n",
    "            \n",
    "            G = 0\n",
    "            visited_state_actions = set()\n",
    "            \n",
    "            for t in range(len(episode_data)-1, -1, -1):\n",
    "                state, action, reward = episode_data[t]\n",
    "                G = self.gamma * G + reward\n",
    "                \n",
    "                state_action = (state, action)\n",
    "                if state_action not in visited_state_actions:\n",
    "                    visited_state_actions.add(state_action)\n",
    "                    \n",
    "                    self.returns_sum[state_action] += G\n",
    "                    self.returns_count[state_action] += 1\n",
    "                    self.Q[state][action] = self.returns_sum[state_action] / self.returns_count[state_action]\n",
    "                    \n",
    "                    self.update_policy(state)\n",
    "        \n",
    "        return episode_rewards, episode_lengths\n",
    "    \n",
    "    def evaluate(self, num_episodes=10, render=False):\n",
    "        render_mode = \"human\" if render else None\n",
    "        if render:\n",
    "            self.env.close()\n",
    "            eval_env = DiscretizedAcrobot(bins=self.env.bins, render_mode=render_mode)\n",
    "        else:\n",
    "            eval_env = self.env\n",
    "            \n",
    "        total_rewards = 0\n",
    "        episode_lengths = []\n",
    "        \n",
    "        for _ in range(num_episodes):\n",
    "            state, _ = eval_env.reset()\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            done = False\n",
    "            truncated = False\n",
    "            \n",
    "            while not (done or truncated):\n",
    "                action = self.select_action(state, exploring=False)\n",
    "                next_state, reward, done, truncated, _ = eval_env.step(action)\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                steps += 1\n",
    "                \n",
    "                if render:\n",
    "                    eval_env.render()\n",
    "                    time.sleep(0.01)\n",
    "            \n",
    "            total_rewards += episode_reward\n",
    "            episode_lengths.append(steps)\n",
    "        \n",
    "        if render:\n",
    "            eval_env.close()\n",
    "            \n",
    "        avg_reward = total_rewards / num_episodes\n",
    "        avg_length = np.mean(episode_lengths)\n",
    "        return avg_reward, avg_length\n",
    "    \n",
    "    def plot_results(self, rewards, lengths):\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n",
    "        \n",
    "        ax1.plot(rewards)\n",
    "        ax1.set_title('Learning Curve - Rewards')\n",
    "        ax1.set_xlabel('Episode')\n",
    "        ax1.set_ylabel('Episode Reward')\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        window = 50\n",
    "        if len(rewards) >= window:\n",
    "            rolling_mean = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "            ax1.plot(range(window-1, len(rewards)), rolling_mean, 'r-', linewidth=2, \n",
    "                     label=f'{window}-episode Moving Avg')\n",
    "            ax1.legend()\n",
    "        \n",
    "        ax2.plot(lengths)\n",
    "        ax2.set_title('Learning Curve - Episode Lengths')\n",
    "        ax2.set_xlabel('Episode')\n",
    "        ax2.set_ylabel('Steps per Episode')\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        if len(lengths) >= window:\n",
    "            rolling_mean = np.convolve(lengths, np.ones(window)/window, mode='valid')\n",
    "            ax2.plot(range(window-1, len(lengths)), rolling_mean, 'r-', linewidth=2, \n",
    "                     label=f'{window}-episode Moving Avg')\n",
    "            ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def main():\n",
    "    env = DiscretizedAcrobot(bins=8)\n",
    "    \n",
    "    agent = MonteCarloAgent(env, epsilon=0.2, gamma=0.99)\n",
    "    \n",
    "    print(\"Running Monte Carlo Prediction on initial policy...\")\n",
    "    agent.monte_carlo_prediction(num_episodes=100)\n",
    "    \n",
    "    print(\"\\nTraining with Monte Carlo Control...\")\n",
    "    rewards, lengths = agent.monte_carlo_control(num_episodes=2000)\n",
    "    \n",
    "    agent.plot_results(rewards, lengths)\n",
    "    \n",
    "    print(\"\\nEvaluating learned policy...\")\n",
    "    avg_reward, avg_length = agent.evaluate(num_episodes=10, render=True)\n",
    "    print(f\"Average Reward: {avg_reward:.2f}\")\n",
    "    print(f\"Average Episode Length: {avg_length:.1f}\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd5a28f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
