{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76e6d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "gym.register_envs(gymnasium_robotics)\n",
    "\n",
    "class DiscretizedFetchPickAndPlace:\n",
    "    def __init__(self, bins=5, render_mode=None):\n",
    "        self.env = gym.make(\"FetchPickAndPlace-v3\", render_mode=render_mode)\n",
    "        self.bins = bins\n",
    "        self.action_bins = 3\n",
    "        self.num_actions = self.action_bins ** 4\n",
    "        obs, _ = self.env.reset()\n",
    "        self.observation_keys = list(obs.keys())\n",
    "        \n",
    "    def reset(self, seed=None):\n",
    "        obs, info = self.env.reset(seed=seed)\n",
    "        return self._discretize_observation(obs), info\n",
    "    \n",
    "    def step(self, discrete_action):\n",
    "        continuous_action = self._discrete_to_continuous_action(discrete_action)\n",
    "        obs, reward, terminated, truncated, info = self.env.step(continuous_action)\n",
    "        return self._discretize_observation(obs), reward, terminated, truncated, info\n",
    "    \n",
    "    def _discretize_observation(self, obs):\n",
    "        grip_pos = obs['observation'][:3]\n",
    "        object_pos = obs['observation'][3:6]\n",
    "        target_pos = obs['desired_goal']\n",
    "        grip_to_obj = np.linalg.norm(grip_pos - object_pos)\n",
    "        obj_to_target = np.linalg.norm(object_pos - target_pos)\n",
    "        discrete_grip_obj = self._discretize_value(grip_to_obj, 0, 1.0, self.bins)\n",
    "        discrete_obj_target = self._discretize_value(obj_to_target, 0, 1.0, self.bins)\n",
    "        discrete_gripper_pos = tuple(self._discretize_value(grip_pos[i], -1, 1, self.bins) for i in range(3))\n",
    "        state = (discrete_grip_obj, discrete_obj_target) + discrete_gripper_pos\n",
    "        return state\n",
    "    \n",
    "    def _discretize_value(self, value, min_val, max_val, bins):\n",
    "        bin_size = (max_val - min_val) / bins\n",
    "        discretized = int((value - min_val) / bin_size)\n",
    "        return min(bins - 1, max(0, discretized))\n",
    "    \n",
    "    def _discrete_to_continuous_action(self, discrete_action):\n",
    "        actions = []\n",
    "        temp = discrete_action\n",
    "        for _ in range(4):\n",
    "            actions.insert(0, temp % self.action_bins)\n",
    "            temp = temp // self.action_bins\n",
    "        continuous_action = np.zeros(4)\n",
    "        for i in range(4):\n",
    "            if i < 3:\n",
    "                continuous_action[i] = -1.0 + (actions[i] * 2.0 / (self.action_bins - 1))\n",
    "            else:\n",
    "                continuous_action[i] = 0.0 if actions[i] == 0 else 1.0\n",
    "        return continuous_action\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "class MonteCarloAgent:\n",
    "    def __init__(self, env, epsilon=0.2, gamma=0.95):\n",
    "        self.env = env\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.num_actions = env.num_actions\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.num_actions))\n",
    "        self.returns_count = defaultdict(lambda: np.zeros(self.num_actions))\n",
    "        self.returns_sum = defaultdict(lambda: np.zeros(self.num_actions))\n",
    "        self.policy = defaultdict(lambda: np.ones(self.num_actions) / self.num_actions)\n",
    "    \n",
    "    def select_action(self, state, exploring=True):\n",
    "        if exploring and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            return np.argmax(self.Q[state])\n",
    "    \n",
    "    def update_policy(self, state):\n",
    "        best_action = np.argmax(self.Q[state])\n",
    "        for a in range(self.num_actions):\n",
    "            self.policy[state][a] = 1.0 if a == best_action else 0.0\n",
    "    \n",
    "    def generate_episode(self, max_steps=50, exploring=True):\n",
    "        episode = []\n",
    "        state, _ = self.env.reset()\n",
    "        for _ in range(max_steps):\n",
    "            action = self.select_action(state, exploring)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            state = next_state\n",
    "        return episode\n",
    "    \n",
    "    def monte_carlo_prediction(self, num_episodes=100):\n",
    "        for _ in range(num_episodes):\n",
    "            episode = self.generate_episode()\n",
    "            G = 0\n",
    "            visited_state_actions = set()\n",
    "            for t in range(len(episode)-1, -1, -1):\n",
    "                state, action, reward = episode[t]\n",
    "                G = self.gamma * G + reward\n",
    "                if (state, action) not in visited_state_actions:\n",
    "                    visited_state_actions.add((state, action))\n",
    "                    self.returns_sum[(state, action)] += G\n",
    "                    self.returns_count[(state, action)] += 1\n",
    "                    self.Q[state][action] = self.returns_sum[(state, action)] / self.returns_count[(state, action)]\n",
    "    \n",
    "    def monte_carlo_control(self, num_episodes=1000):\n",
    "        episode_rewards = []\n",
    "        for episode in range(num_episodes):\n",
    "            episode_data = self.generate_episode(exploring=True)\n",
    "            episode_rewards.append(sum(r for _, _, r in episode_data))\n",
    "            if episode % 50 == 0:\n",
    "                print(f\"Episode {episode}/{num_episodes}, Avg Reward: {np.mean(episode_rewards[-50:] if episode > 0 else episode_rewards):.2f}\")\n",
    "            G = 0\n",
    "            visited_state_actions = set()\n",
    "            for t in range(len(episode_data)-1, -1, -1):\n",
    "                state, action, reward = episode_data[t]\n",
    "                G = self.gamma * G + reward\n",
    "                if (state, action) not in visited_state_actions:\n",
    "                    visited_state_actions.add((state, action))\n",
    "                    self.returns_sum[(state, action)] += G\n",
    "                    self.returns_count[(state, action)] += 1\n",
    "                    self.Q[state][action] = self.returns_sum[(state, action)] / self.returns_count[(state, action)]\n",
    "                    self.update_policy(state)\n",
    "        return episode_rewards\n",
    "    \n",
    "    def evaluate(self, num_episodes=5, render=False):\n",
    "        render_mode = \"human\" if render else None\n",
    "        if render:\n",
    "            self.env.close()\n",
    "            eval_env = DiscretizedFetchPickAndPlace(render_mode=render_mode)\n",
    "        else:\n",
    "            eval_env = self.env\n",
    "        total_rewards = 0\n",
    "        success_count = 0\n",
    "        for _ in range(num_episodes):\n",
    "            state, _ = eval_env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            truncated = False\n",
    "            for step in range(100):\n",
    "                action = self.select_action(state, exploring=False)\n",
    "                next_state, reward, done, truncated, _ = eval_env.step(action)\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                if render:\n",
    "                    time.sleep(0.05)\n",
    "                if done or truncated:\n",
    "                    if reward > 0:\n",
    "                        success_count += 1\n",
    "                    break\n",
    "            total_rewards += episode_reward\n",
    "        if render:\n",
    "            eval_env.close()\n",
    "        avg_reward = total_rewards / num_episodes\n",
    "        success_rate = success_count / num_episodes\n",
    "        return avg_reward, success_rate\n",
    "    \n",
    "    def plot_learning_curve(self, rewards):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(rewards)\n",
    "        plt.title('Learning Curve - Monte Carlo Control')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Episode Reward')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "def main():\n",
    "    env = DiscretizedFetchPickAndPlace(bins=5)\n",
    "    agent = MonteCarloAgent(env, epsilon=0.2, gamma=0.95)\n",
    "    print(\"Training Monte Carlo control agent...\")\n",
    "    rewards = agent.monte_carlo_control(num_episodes=1000)\n",
    "    agent.plot_learning_curve(rewards)\n",
    "    print(\"\\nEvaluating learned policy...\")\n",
    "    avg_reward, success_rate = agent.evaluate(num_episodes=10, render=True)\n",
    "    print(f\"Average Reward: {avg_reward:.2f}\")\n",
    "    print(f\"Success Rate: {success_rate:.2f}\")\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
