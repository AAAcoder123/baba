{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e60bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "class ArmSimulator:\n",
    "    def __init__(self, segment_sizes=[0.9, 0.7], goal_position=None):\n",
    "        self.segment_sizes = np.array(segment_sizes)\n",
    "        self.joint_count = len(segment_sizes)\n",
    "        self.rotation_increment = 0.15\n",
    "        self.angle_bounds = (-np.pi, np.pi)\n",
    "        self.joint_angles = np.zeros(self.joint_count)\n",
    "        self.goal_position = np.array([0.9, 0.7]) if goal_position is None else np.array(goal_position)\n",
    "        self.max_reach = np.sum(self.segment_sizes)\n",
    "        \n",
    "    def initialize(self):\n",
    "        self.joint_angles = np.random.uniform(self.angle_bounds[0], self.angle_bounds[1], self.joint_count)\n",
    "        return self._discretize_state()\n",
    "    \n",
    "    def execute_action(self, action_id):\n",
    "        joint_id = action_id // 2\n",
    "        direction = 1 if action_id % 2 == 0 else -1\n",
    "        \n",
    "        if 0 <= joint_id < self.joint_count:\n",
    "            delta = direction * self.rotation_increment\n",
    "            self.joint_angles[joint_id] += delta\n",
    "            self.joint_angles[joint_id] = np.clip(self.joint_angles[joint_id], \n",
    "                                                self.angle_bounds[0], \n",
    "                                                self.angle_bounds[1])\n",
    "        \n",
    "        end_effector = self._compute_position(self.joint_angles)\n",
    "        distance = np.linalg.norm(end_effector - self.goal_position)\n",
    "        \n",
    "        reward = -distance\n",
    "        terminal = distance < 0.15\n",
    "        \n",
    "        if terminal:\n",
    "            reward += 15.0\n",
    "            \n",
    "        return self._discretize_state(), reward, terminal\n",
    "    \n",
    "    def _discretize_state(self):\n",
    "        discretization = 0.25\n",
    "        discrete_angles = tuple((self.joint_angles / discretization).astype(int))\n",
    "        return discrete_angles\n",
    "    \n",
    "    def _compute_position(self, angles):\n",
    "        position = np.zeros(2)\n",
    "        cumulative_angle = 0\n",
    "        \n",
    "        for i in range(self.joint_count):\n",
    "            cumulative_angle += angles[i]\n",
    "            position[0] += self.segment_sizes[i] * np.cos(cumulative_angle)\n",
    "            position[1] += self.segment_sizes[i] * np.sin(cumulative_angle)\n",
    "            \n",
    "        return position\n",
    "    \n",
    "    def visualize(self):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        \n",
    "        position = np.zeros(2)\n",
    "        cumulative_angle = 0\n",
    "        coordinates = [position.copy()]\n",
    "        \n",
    "        for i in range(self.joint_count):\n",
    "            cumulative_angle += self.joint_angles[i]\n",
    "            position[0] += self.segment_sizes[i] * np.cos(cumulative_angle)\n",
    "            position[1] += self.segment_sizes[i] * np.sin(cumulative_angle)\n",
    "            coordinates.append(position.copy())\n",
    "        \n",
    "        coordinates = np.array(coordinates)\n",
    "        \n",
    "        plt.plot(coordinates[:, 0], coordinates[:, 1], 'b-', linewidth=4)\n",
    "        plt.plot(coordinates[:, 0], coordinates[:, 1], 'ko', markersize=8)\n",
    "        plt.plot(self.goal_position[0], self.goal_position[1], 'r*', markersize=16)\n",
    "        \n",
    "        workspace_radius = self.max_reach * 1.2\n",
    "        plt.xlim([-workspace_radius, workspace_radius])\n",
    "        plt.ylim([-workspace_radius, workspace_radius])\n",
    "        plt.grid(True)\n",
    "        plt.title('Articulated Arm Simulation')\n",
    "        plt.xlabel('X Position')\n",
    "        plt.ylabel('Y Position')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class MonteCarloLearner:\n",
    "    def __init__(self, simulator, exploration_rate=0.15, discount_factor=0.92):\n",
    "        self.simulator = simulator\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.action_count = simulator.joint_count * 2\n",
    "        self.state_action_values = defaultdict(lambda: np.zeros(self.action_count))\n",
    "        self.returns_sum = defaultdict(lambda: np.zeros(self.action_count))\n",
    "        self.returns_count = defaultdict(lambda: np.zeros(self.action_count))\n",
    "        self.behavior_policy = {}\n",
    "        \n",
    "    def select_action(self, state, explore=True):\n",
    "        if state not in self.behavior_policy:\n",
    "            self.behavior_policy[state] = np.ones(self.action_count) / self.action_count\n",
    "        \n",
    "        if explore and np.random.random() < self.exploration_rate:\n",
    "            return np.random.randint(self.action_count)\n",
    "        else:\n",
    "            return np.argmax(self.state_action_values[state])\n",
    "    \n",
    "    def collect_trajectory(self, max_steps=150, explore=True):\n",
    "        trajectory = []\n",
    "        state = self.simulator.initialize()\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            action = self.select_action(state, explore)\n",
    "            next_state, reward, done = self.simulator.execute_action(action)\n",
    "            trajectory.append((state, action, reward))\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        return trajectory\n",
    "    \n",
    "    def update_behavior_policy(self, state):\n",
    "        best_action = np.argmax(self.state_action_values[state])\n",
    "        self.behavior_policy[state] = np.zeros(self.action_count)\n",
    "        self.behavior_policy[state][best_action] = 1.0\n",
    "    \n",
    "    def evaluate_policy(self, episodes=500):\n",
    "        for _ in range(episodes):\n",
    "            trajectory = self.collect_trajectory(explore=True)\n",
    "            visited_pairs = set()\n",
    "            \n",
    "            returns = 0\n",
    "            for t in range(len(trajectory)-1, -1, -1):\n",
    "                state, action, reward = trajectory[t]\n",
    "                returns = self.discount_factor * returns + reward\n",
    "                \n",
    "                if (state, action) not in visited_pairs:\n",
    "                    visited_pairs.add((state, action))\n",
    "                    self.returns_sum[(state, action)] += returns\n",
    "                    self.returns_count[(state, action)] += 1\n",
    "                    self.state_action_values[state][action] = (\n",
    "                        self.returns_sum[(state, action)] / self.returns_count[(state, action)]\n",
    "                    )\n",
    "    \n",
    "    def improve_policy(self, episodes=3000):\n",
    "        for i in range(episodes):\n",
    "            trajectory = self.collect_trajectory(explore=True)\n",
    "            visited_pairs = set()\n",
    "            \n",
    "            returns = 0\n",
    "            for t in range(len(trajectory)-1, -1, -1):\n",
    "                state, action, reward = trajectory[t]\n",
    "                returns = self.discount_factor * returns + reward\n",
    "                \n",
    "                if (state, action) not in visited_pairs:\n",
    "                    visited_pairs.add((state, action))\n",
    "                    self.returns_sum[(state, action)] += returns\n",
    "                    self.returns_count[(state, action)] += 1\n",
    "                    self.state_action_values[state][action] = (\n",
    "                        self.returns_sum[(state, action)] / self.returns_count[(state, action)]\n",
    "                    )\n",
    "                    self.update_behavior_policy(state)\n",
    "            \n",
    "            if (i+1) % 300 == 0:\n",
    "                print(f\"Training progress: {i+1}/{episodes} episodes completed\")\n",
    "                self.assess_performance()\n",
    "    \n",
    "    def assess_performance(self, trials=5, steps_limit=100, show_visualization=False):\n",
    "        success_counter = 0\n",
    "        reward_total = 0\n",
    "        \n",
    "        for _ in range(trials):\n",
    "            state = self.simulator.initialize()\n",
    "            episode_reward = 0\n",
    "            terminal = False\n",
    "            \n",
    "            for _ in range(steps_limit):\n",
    "                action = self.select_action(state, explore=False)\n",
    "                next_state, reward, terminal = self.simulator.execute_action(action)\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                \n",
    "                if terminal:\n",
    "                    success_counter += 1\n",
    "                    break\n",
    "            \n",
    "            reward_total += episode_reward\n",
    "        \n",
    "        if show_visualization:\n",
    "            self.simulator.visualize()\n",
    "            \n",
    "        avg_reward = reward_total / trials\n",
    "        success_rate = success_counter / trials\n",
    "        print(f\"Performance metrics - Average reward: {avg_reward:.2f}, Success rate: {success_rate:.2f}\")\n",
    "        \n",
    "        return avg_reward, success_rate\n",
    "\n",
    "\n",
    "def run_experiment():\n",
    "    arm_simulator = ArmSimulator(segment_sizes=[1.1, 0.75], goal_position=[1.2, 0.5])\n",
    "    \n",
    "    mc_agent = MonteCarloLearner(arm_simulator, exploration_rate=0.25, discount_factor=0.9)\n",
    "    \n",
    "    print(\"Starting Monte Carlo training procedure...\")\n",
    "    mc_agent.improve_policy(episodes=4000)\n",
    "    \n",
    "    print(\"\\nFinal performance evaluation:\")\n",
    "    mc_agent.assess_performance(trials=10, show_visualization=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_experiment()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
